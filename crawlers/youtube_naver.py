"""ìœ íŠœë¸Œ ë„¤ì´ë²„ ì»¨í¼ëŸ°ìŠ¤ ì˜ìƒ í¬ë¡¤ëŸ¬"""
import os
import sys
import json
from datetime import datetime, timezone
import feedparser

# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.rss_generator import create_rss_feed
from utils.logger import CrawlLogger


def load_config():
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'config.json'
    )
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def crawl_youtube_channel(channel_id, filter_keywords=None, exclude_shorts=False):
    """
    ìœ íŠœë¸Œ ì±„ë„ RSSë¥¼ ê°€ì ¸ì™€ì„œ í‚¤ì›Œë“œ í•„í„°ë§

    Args:
        channel_id: ìœ íŠœë¸Œ ì±„ë„ ID
        filter_keywords: í•„í„°ë§í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        exclude_shorts: ì‡¼ì¸  ì œì™¸ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)

    Returns:
        list: í•„í„°ë§ëœ ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    print(f"ìœ íŠœë¸Œ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘... (channel_id: {channel_id})")

    # ìœ íŠœë¸Œ ì±„ë„ RSS URL
    rss_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'
    print(f"RSS URL: {rss_url}")

    # RSS íŒŒì‹±
    feed = feedparser.parse(rss_url)

    if not feed.entries:
        raise Exception("í”¼ë“œì—ì„œ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì±„ë„ IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    print(f"âœ… ì´ {len(feed.entries)}ê°œ ì˜ìƒ ë°œê²¬")

    videos = []

    for entry in feed.entries:
        title = entry.get('title', '')
        link = entry.get('link', '')
        summary = entry.get('summary', '')
        author = entry.get('author', 'Unknown')

        # ì‡¼ì¸  ì œì™¸ ì˜µì…˜ ì²´í¬
        if exclude_shorts and '/shorts/' in link:
            continue

        # ë‚ ì§œ íŒŒì‹± (timezone ì •ë³´ í¬í•¨)
        published = entry.get('published_parsed')
        if published:
            date = datetime(*published[:6], tzinfo=timezone.utc)
        else:
            date = datetime.now(timezone.utc)

        # í‚¤ì›Œë“œ í•„í„°ë§
        if filter_keywords:
            # ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            title_lower = title.lower()

            # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€
            matched = False
            for keyword in filter_keywords:
                if keyword.lower() in title_lower:
                    matched = True
                    print(f"  âœ… {title}")
                    break

            if matched:
                videos.append({
                    'title': title,
                    'link': link,
                    'summary': summary,
                    'author': author,
                    'date': date
                })
        else:
            # í•„í„°ë§ ì—†ì´ ëª¨ë‘ ì¶”ê°€
            videos.append({
                'title': title,
                'link': link,
                'summary': summary,
                'author': author,
                'date': date
            })

    if filter_keywords:
        print(f"\nğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(videos)}ê°œ ì˜ìƒ (ì „ì²´ {len(feed.entries)}ê°œ ì¤‘)")
    else:
        print(f"âœ… {len(videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ")

    return videos


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = CrawlLogger()

    try:
        # ì„¤ì • ë¡œë“œ
        config = load_config()
        feed_config = config['feeds']['naver_conference']

        # í¬ë¡¤ë§ ì‹¤í–‰
        channel_id = feed_config['channel_id']
        filter_keywords = feed_config.get('filter_keywords', [])
        exclude_shorts = feed_config.get('exclude_shorts', False)

        videos = crawl_youtube_channel(channel_id, filter_keywords, exclude_shorts)

        # RSS ìƒì„±
        feed_info = {
            'title': feed_config['name'],
            'link': f'https://www.youtube.com/channel/{channel_id}',
            'description': feed_config['description']
        }

        output_path = f"docs/{feed_config['output']}"
        os.makedirs('docs', exist_ok=True)

        create_rss_feed(feed_info, videos, output_path)

        # ì„±ê³µ ë¡œê·¸
        if videos:
            logger.log_success(
                'naver_conference',
                len(videos),
                f'{output_path} ìƒì„± ì™„ë£Œ (í•„í„°: {", ".join(filter_keywords)})'
            )
            print(f"\nâœ… RSS í”¼ë“œ ìƒì„± ì™„ë£Œ: {output_path}")
        else:
            # ì˜ìƒì´ ì—†ì–´ë„ ì„±ê³µìœ¼ë¡œ ê¸°ë¡ (ê²½ê³  ë©”ì‹œì§€ í¬í•¨)
            logger.log_success(
                'naver_conference',
                0,
                f'âš ï¸ í•„í„°ë§ëœ ì˜ìƒ ì—†ìŒ - ë¹ˆ RSS ìƒì„±: {output_path}'
            )
            print(f"\nâš ï¸  í•„í„°ë§ëœ ì˜ìƒì´ ì—†ì–´ ë¹ˆ RSS í”¼ë“œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {output_path}")
            print(f"ğŸ’¡ ë‚˜ì¤‘ì— í‚¤ì›Œë“œì— ë§ëŠ” ì˜ìƒì´ ì—…ë¡œë“œë˜ë©´ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.")

    except Exception as e:
        # ì‹¤íŒ¨ ë¡œê·¸
        logger.log_failure('naver_conference', str(e))
        raise

    finally:
        logger.save()


if __name__ == '__main__':
    main()
