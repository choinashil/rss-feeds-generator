"""Velog íŠ¸ë Œë”© í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
import os
import sys
import re
import json
from datetime import datetime, timezone, timedelta
from playwright.sync_api import sync_playwright
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime

# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.rss_generator import create_rss_feed
from utils.logger import CrawlLogger


def load_config():
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'config.json'
    )
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_existing_pubdates(xml_path):
    """
    ê¸°ì¡´ XML íŒŒì¼ì—ì„œ ê° ì•„ì´í…œì˜ pubDateë¥¼ ì¶”ì¶œ

    Args:
        xml_path: XML íŒŒì¼ ê²½ë¡œ

    Returns:
        dict: {link: pubDate(datetime)} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
    """
    existing_dates = {}

    if not os.path.exists(xml_path):
        return existing_dates

    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # RSS 2.0 í˜•ì‹: channel > item
        for item in root.findall('.//item'):
            link_elem = item.find('link')
            pubdate_elem = item.find('pubDate')

            if link_elem is not None and pubdate_elem is not None:
                link = link_elem.text
                pubdate_str = pubdate_elem.text

                # RFC 2822 í˜•ì‹ì˜ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                try:
                    pubdate = parsedate_to_datetime(pubdate_str)
                    existing_dates[link] = pubdate
                except Exception:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°
                    continue

    except Exception as e:
        print(f"âš ï¸  ê¸°ì¡´ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

    return existing_dates


def parse_velog_date(date_text):
    """
    Velog ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜

    Args:
        date_text: "2025ë…„ 12ì›” 21ì¼" ë˜ëŠ” "6ì¼ ì „" í˜•ì‹

    Returns:
        datetime: UTC timezoneì´ ì ìš©ëœ datetime ê°ì²´
    """
    date_text = date_text.strip()

    # ì ˆëŒ€ ë‚ ì§œ í˜•ì‹: "2025ë…„ 12ì›” 21ì¼"
    absolute_pattern = r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
    match = re.match(absolute_pattern, date_text)
    if match:
        year, month, day = map(int, match.groups())
        return datetime(year, month, day, tzinfo=timezone.utc)

    # ìƒëŒ€ ì‹œê°„ í˜•ì‹: "Xì¼ ì „", "Xì‹œê°„ ì „", "Xë¶„ ì „"
    now = datetime.now(timezone.utc)

    if 'ì¼ ì „' in date_text:
        days = int(re.search(r'(\d+)ì¼', date_text).group(1))
        return now - timedelta(days=days)
    elif 'ì‹œê°„ ì „' in date_text:
        hours = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
        return now - timedelta(hours=hours)
    elif 'ë¶„ ì „' in date_text:
        minutes = int(re.search(r'(\d+)ë¶„', date_text).group(1))
        return now - timedelta(minutes=minutes)
    elif 'ë°©ê¸ˆ' in date_text or 'ì´ˆ ì „' in date_text:
        return now

    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
    return now


def crawl_velog_trending(max_items=20):
    """
    Velog íŠ¸ë Œë”© í˜ì´ì§€ í¬ë¡¤ë§

    Args:
        max_items: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜

    Returns:
        list: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    print("Velog íŠ¸ë Œë”© í¬ë¡¤ë§ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        # Velog íŠ¸ë Œë”© í˜ì´ì§€ ì ‘ì† (week ë‹¨ìœ„)
        page.goto('https://velog.io/trending/week', wait_until='networkidle', timeout=30000)

        # JavaScript ë Œë”ë§ ëŒ€ê¸°
        page.wait_for_selector('h4[class*="PostCard"]', timeout=30000)

        posts = []
        seen_links = set()  # ì¤‘ë³µ ì œê±°

        # í¬ìŠ¤íŠ¸ ì¹´ë“œ(li íƒœê·¸) ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘
        cards = page.query_selector_all('li[class*="PostCard"]')
        print(f"âœ… ì´ {len(cards)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

        for card in cards[:max_items]:
            try:
                # ì œëª© (h4 íƒœê·¸)
                title_elem = card.query_selector('h4[class*="PostCard"]')
                if not title_elem:
                    continue
                title = title_elem.inner_text().strip()

                # ë§í¬ (a íƒœê·¸)
                link_elem = card.query_selector('a[href*="/@"]')
                if not link_elem:
                    continue
                link = link_elem.get_attribute('href')

                # ì „ì²´ URL ë§Œë“¤ê¸°
                if link and not link.startswith('http'):
                    link = f'https://velog.io{link}' if link.startswith('/') else link

                # ì¤‘ë³µ ì²´í¬
                if link in seen_links:
                    continue
                seen_links.add(link)

                # ìš”ì•½ (p.PostCard_clamp___2g_C)
                summary = ''
                summary_elem = card.query_selector('p[class*="PostCard_clamp"]')
                if summary_elem:
                    summary = summary_elem.inner_text().strip()

                # ì‘ì„±ì (footer ì˜ì—­ì˜ b íƒœê·¸)
                author = 'Unknown'
                author_elem = card.query_selector('div[class*="PostCard_footer"] b')
                if author_elem:
                    author = author_elem.inner_text().strip()

                # ë‚ ì§œ (PostCard_subInfo ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ span)
                date = datetime.now(timezone.utc)
                date_elem = card.query_selector('div[class*="PostCard_subInfo"] span')
                if date_elem:
                    date_text = date_elem.inner_text().strip()
                    date = parse_velog_date(date_text)

                posts.append({
                    'title': title,
                    'link': link,
                    'summary': summary[:500] if summary else '',
                    'author': author,
                    'date': date
                })

            except Exception as e:
                print(f"  âš ï¸  ê²Œì‹œê¸€ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        browser.close()

    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(posts)}ê°œ ê²Œì‹œê¸€")
    return posts


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = CrawlLogger()

    try:
        # ì„¤ì • ë¡œë“œ
        config = load_config()
        feed_config = config['feeds']['velog_trending']

        output_path = f"docs/{feed_config['output']}"

        # ê¸°ì¡´ XMLì—ì„œ pubDate ì •ë³´ ë¡œë“œ
        existing_pubdates = load_existing_pubdates(output_path)

        # í¬ë¡¤ë§ ì‹¤í–‰
        posts = crawl_velog_trending(max_items=30)

        if not posts:
            raise Exception("ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")

        # pubDate ì„¤ì •: ê¸°ì¡´ ê¸€ì€ ê¸°ì¡´ ë‚ ì§œ ìœ ì§€, ìƒˆ ê¸€ì€ í˜„ì¬ ì‹œê°„
        current_time = datetime.now(timezone.utc)
        new_count = 0

        for post in posts:
            link = post['link']
            if link in existing_pubdates:
                # ê¸°ì¡´ ê¸€: ê¸°ì¡´ pubDate ìœ ì§€
                post['date'] = existing_pubdates[link]
            else:
                # ìƒˆ ê¸€: í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
                post['date'] = current_time
                new_count += 1

        print(f"âœ¨ ìƒˆë¡œ ì¶”ê°€ëœ ê¸€: {new_count}ê°œ / ê¸°ì¡´ ê¸€: {len(posts) - new_count}ê°œ")

        # RSS ìƒì„±
        feed_info = {
            'title': feed_config['name'],
            'link': 'https://velog.io/trending',
            'description': feed_config['description']
        }

        os.makedirs('docs', exist_ok=True)
        create_rss_feed(feed_info, posts, output_path)

        # ì„±ê³µ ë¡œê·¸
        logger.log_success('velog_trending', len(posts), f'{output_path} ìƒì„± ì™„ë£Œ')

    except Exception as e:
        # ì‹¤íŒ¨ ë¡œê·¸
        logger.log_failure('velog_trending', str(e))
        raise

    finally:
        logger.save()


if __name__ == '__main__':
    main()
